{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:38:50.660119Z",
     "iopub.status.busy": "2025-10-03T05:38:50.659947Z",
     "iopub.status.idle": "2025-10-03T05:40:26.487514Z",
     "shell.execute_reply": "2025-10-03T05:40:26.486440Z",
     "shell.execute_reply.started": "2025-10-03T05:38:50.660103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip3 install bitsandbytes peft trl datasets transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:41:00.723978Z",
     "iopub.status.busy": "2025-10-03T05:41:00.723227Z",
     "iopub.status.idle": "2025-10-03T05:41:30.079417Z",
     "shell.execute_reply": "2025-10-03T05:41:30.078813Z",
     "shell.execute_reply.started": "2025-10-03T05:41:00.723942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:57:14.863431Z",
     "iopub.status.busy": "2025-10-03T05:57:14.862425Z",
     "iopub.status.idle": "2025-10-03T05:57:14.871325Z",
     "shell.execute_reply": "2025-10-03T05:57:14.870182Z",
     "shell.execute_reply.started": "2025-10-03T05:57:14.863388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Replaced by report_to=\"none\"\n",
    "\n",
    "# Set matrix multiplication precision to avoid BFloat16\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"  # base model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_SEQ_LEN = 512  # Increased to avoid truncation error\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:41:40.875284Z",
     "iopub.status.busy": "2025-10-03T05:41:40.874938Z",
     "iopub.status.idle": "2025-10-03T05:41:40.900110Z",
     "shell.execute_reply": "2025-10-03T05:41:40.899319Z",
     "shell.execute_reply.started": "2025-10-03T05:41:40.875240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/test-kaggle-json/test_kaggle.json\", \"r\") as f:\n",
    "    train_raw = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/train-kaggle-json/train_kaggle.json\", \"r\") as f:\n",
    "    test_raw = json.load(f)\n",
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM).\n",
    "Analyze the facial expression in the given image and output the emotion.\n",
    "Possible emotions: Natural, anger, fear, joy, sadness, surprise.\"\"\"\n",
    "#/kaggle/input/autistic-children-emotions-dr-fatma-m-talaat/Autistic Children Emotions - Dr. Fatma M. Talaat\n",
    "def format_sample(sample):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        sample[\"messages\"][0],  # user: image + text\n",
    "        sample[\"messages\"][1],  # assistant: label\n",
    "    ]\n",
    "\n",
    "train_dataset = [format_sample(s) for s in train_raw]\n",
    "test_dataset = [format_sample(s) for s in test_raw]\n",
    "eval_dataset = train_dataset[:10]  # Small eval subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:41:43.957318Z",
     "iopub.status.busy": "2025-10-03T05:41:43.957024Z",
     "iopub.status.idle": "2025-10-03T05:45:11.921730Z",
     "shell.execute_reply": "2025-10-03T05:45:11.920566Z",
     "shell.execute_reply.started": "2025-10-03T05:41:43.957295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e673732089494a47ad9f818d243733ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb2851297b4241b211c027c060cb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a5048ce254e4e9a25c545dd611359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a385c5b661f49629592074ceab9e75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e544d8801d2947e6b27f305d5a44de77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae77db7f87d44a8bca0be4f7fc2a2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be27cb256e384d948b8e0edf358bed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75fab64369448a8a8a5328b8cff04c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e5133cedf3493fa0b516fe7e7605b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2041e807a9014a79ad880be2f12680db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42430081424b4ea08515179e11e2778a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d3629414d4659805fc823806ef1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ec47a276942aab81dfa69e7075f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363f30da0e174bacb0ffa61a9ab0f0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335f2c6677654a2ca4c3d54b02b10391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89866ccbe86d4be4948ea2d1d66b9af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor with 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use float16 to avoid BFloat16 mismatch\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.float16,  # Ensure float16 for computations\n",
    "    device_map=\"auto\",  # Still use for multi-GPU if available, but quantization avoids meta tensors\n",
    "    use_cache=False\n",
    ")\n",
    "model.config.torch_dtype = torch.float16  # Ensure float16 in config\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:45:22.377270Z",
     "iopub.status.busy": "2025-10-03T05:45:22.376464Z",
     "iopub.status.idle": "2025-10-03T05:45:22.516167Z",
     "shell.execute_reply": "2025-10-03T05:45:22.515335Z",
     "shell.execute_reply.started": "2025-10-03T05:45:22.377240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,293,898,752 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T06:00:24.080675Z",
     "iopub.status.busy": "2025-10-03T06:00:24.080029Z",
     "iopub.status.idle": "2025-10-03T06:00:25.132419Z",
     "shell.execute_reply": "2025-10-03T06:00:25.131733Z",
     "shell.execute_reply.started": "2025-10-03T06:00:24.080650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=True,  # Re-enabled with quantization (safer now)\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1,\n",
    "    warmup_steps=0,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"none\",  # Fix WANDB warning\n",
    "    gradient_accumulation_steps=4  # Manage memory\n",
    ")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))  # Resize to reduce token count\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "    image_inputs = [preprocess_image(example[1][\"content\"][0][\"image\"]) for example in examples]\n",
    "    image_inputs = [img for img in image_inputs if img is not None]  # Filter out failed images\n",
    "\n",
    "    if not image_inputs: \n",
    "        raise ValueError(\"No valid images in batch\")\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts[:len(image_inputs)],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        max_length=None\n",
    "    )\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    batch[\"labels\"][batch[\"labels\"] == processor.tokenizer.pad_token_id] = -100\n",
    "    return batch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:51:06.638172Z",
     "iopub.status.busy": "2025-10-03T05:51:06.637870Z",
     "iopub.status.idle": "2025-10-03T05:51:06.648489Z",
     "shell.execute_reply": "2025-10-03T05:51:06.647637Z",
     "shell.execute_reply.started": "2025-10-03T05:51:06.638150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(sample_data, model, processor, max_new_tokens=20):\n",
    "    #print(\"debugging\")\n",
    "    try:\n",
    "        user_content = sample_data[1][\"content\"]\n",
    "        image_path = user_content[0][\"image\"]\n",
    "        query_text = user_content[1][\"text\"]\n",
    "        actual_answer = sample_data[2][\"content\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting sample_data:\", e)\n",
    "        return None, None\n",
    "\n",
    "    # print(\"Image path:\", image_path)\n",
    "    # print(\"Query text:\", query_text)\n",
    "    # print(\"Actual answer:\", actual_answer)\n",
    "\n",
    "    # System message\n",
    "    system_msg = (\n",
    "        \"You are a highly advanced Vision Language Model (VLM). \"\n",
    "        \"Analyze the facial expression in the given image and output ONLY the emotion name. Do not add any extra text. \"\n",
    "        \"Possible emotions: Natural, anger, fear, joy, sadness, surprise.\"\n",
    "    )\n",
    "\n",
    "    # Build prompt\n",
    "    prompt_text = processor.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_msg}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image_path}, {\"type\": \"text\", \"text\": query_text}]}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Load image and prepare inputs\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}:\", e)\n",
    "        return None, None\n",
    "\n",
    "    inputs = processor(text=[prompt_text], images=[image], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Print model's datatype\n",
    "    # model_dtype = next(model.parameters()).dtype\n",
    "    # print(\"Model dtype:\", model_dtype)\n",
    "\n",
    "    # Cast floats to float16 (consistent with quantization)\n",
    "    for k, v in inputs.items():\n",
    "        if torch.is_floating_point(v):\n",
    "            inputs[k] = v.to(dtype=torch.float16)\n",
    "        #print(f\"Input tensor '{k}' dtype after move:\", inputs[k].dtype)\n",
    "\n",
    "    # Generate output with greedy decoding\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Greedy decoding for clean output\n",
    "                temperature=0.1,  # Low temperature for deterministic output\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "            # Decode and extract only the assistant's response\n",
    "            output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            # Extract after last \"assistant\" tag\n",
    "            if \"assistant\" in output_text:\n",
    "                output_text = output_text.split(\"assistant\")[-1].strip()\n",
    "            output_text = output_text.split()[0].strip() if output_text.split() else output_text\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        output_text = None\n",
    "\n",
    "    return output_text, actual_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T06:00:37.281641Z",
     "iopub.status.busy": "2025-10-03T06:00:37.280987Z",
     "iopub.status.idle": "2025-10-03T06:29:41.181295Z",
     "shell.execute_reply": "2025-10-03T06:29:41.180663Z",
     "shell.execute_reply.started": "2025-10-03T06:00:37.281616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 28:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.819500</td>\n",
       "      <td>8.526506</td>\n",
       "      <td>3.128513</td>\n",
       "      <td>31739.000000</td>\n",
       "      <td>0.359292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.116400</td>\n",
       "      <td>7.798287</td>\n",
       "      <td>3.706689</td>\n",
       "      <td>63319.000000</td>\n",
       "      <td>0.367461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.222500</td>\n",
       "      <td>6.655527</td>\n",
       "      <td>4.689507</td>\n",
       "      <td>95060.000000</td>\n",
       "      <td>0.373742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=190, training_loss=7.691737365722656, metrics={'train_runtime': 1743.2957, 'train_samples_per_second': 0.43, 'train_steps_per_second': 0.109, 'total_flos': 5589902474618880.0, 'train_loss': 7.691737365722656, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:15:23.383742Z",
     "iopub.status.busy": "2025-10-02T16:15:23.383479Z",
     "iopub.status.idle": "2025-10-02T16:15:23.387280Z",
     "shell.execute_reply": "2025-10-02T16:15:23.386530Z",
     "shell.execute_reply.started": "2025-10-02T16:15:23.383723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "# zip_path = \"/kaggle/input/archive-zip\"\n",
    "# extract_dir = \"/kaggle/input/dataset\"\n",
    "# os.makedirs(extract_dir, exist_ok=True)\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "# print(\"Files unzipped to:\", extract_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2908203,
     "sourceId": 5011894,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391595,
     "sourceId": 13243518,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391636,
     "sourceId": 13243583,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391640,
     "sourceId": 13243588,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391719,
     "sourceId": 13243692,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391724,
     "sourceId": 13243697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
